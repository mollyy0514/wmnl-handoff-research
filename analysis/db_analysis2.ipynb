{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import swifter\n",
    "import json\n",
    "import itertools\n",
    "from pprint import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files\n",
    "# This function input the path of experiment directory and output a list of device directories of the experiment directory.\n",
    "def find_device_under_exp(exp_dir_path):\n",
    "    dev_dir_list = sorted([os.path.join(exp_dir_path, d) for d in os.listdir(exp_dir_path) if d.startswith('qc') or d.startswith('sm')])\n",
    "    return dev_dir_list\n",
    "\n",
    "def find_trace_under_device(dev_dir_path):\n",
    "    trace_dir_list = sorted([os.path.join(dev_dir_path, d) for d in os.listdir(dev_dir_path)])\n",
    "    return trace_dir_list\n",
    "\n",
    "# Statistic functions\n",
    "# This function input the file path of the loss_latency csv and output the loss and excessive latency rate.\n",
    "def count_loss_excl_rate(file_path):\n",
    "\n",
    "    df = pd.read_csv (file_path)\n",
    "\n",
    "    # Total package in the experiment\n",
    "    total_pkg_num = len(df)\n",
    "\n",
    "    # Loss calculate\n",
    "    loss_cond = df['lost'] == True\n",
    "    loss_num = loss_cond.value_counts().loc[True]\n",
    "    loss_rate = loss_num/total_pkg_num\n",
    "\n",
    "    # Excexxive latency calculate\n",
    "    exc_lat = 0.1\n",
    "    excl_cond = df[loss_cond==False]['latency'] > exc_lat\n",
    "    excl_num = excl_cond.value_counts().loc[True]\n",
    "    excl_rate = excl_num/total_pkg_num\n",
    "\n",
    "    return loss_rate, excl_rate\n",
    "\n",
    "# This function input two file paths of the loss_latency csv and output the \n",
    "# loss and excessive latency rate of dual radio condition.\n",
    "def count_loss_excl_rate_dual(file_path1, file_path2):\n",
    "\n",
    "    df1 = pd.read_csv(file_path1)\n",
    "    df2 = pd.read_csv(file_path2)\n",
    "\n",
    "    start_seq = df1['seq'].iloc[0] if df1['seq'].iloc[0] >=  df2['seq'].iloc[0] else df2['seq'].iloc[0]\n",
    "    end_seq = df1['seq'].iloc[-1] if df1['seq'].iloc[-1] <=  df2['seq'].iloc[-1] else df2['seq'].iloc[-1]\n",
    "    total_pkg_num = end_seq - start_seq + 1\n",
    "\n",
    "    cond1 = (df1['seq'] >= start_seq) & (df1['seq'] <= end_seq)\n",
    "    df1 = df1[cond1]\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    cond2 = (df2['seq'] >= start_seq) & (df2['seq'] <= end_seq)\n",
    "    df2 = df2[cond2]\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "\n",
    "    # Loss calculate for dual radios redundant packets.\n",
    "    loss_cond = (df1['lost'] == True) & (df2['lost'] == True)\n",
    "    try: loss_num = loss_cond.value_counts().loc[True]\n",
    "    except: loss_num = 0\n",
    "    loss_rate = loss_num/total_pkg_num\n",
    "\n",
    "    # Excexxive latency calculate for dual radios redundant packets.\n",
    "    exc_lat = 0.1   \n",
    "    excl_cond1 = df1[(loss_cond==False)]['latency'] > exc_lat\n",
    "    excl_cond2 = df2[(loss_cond==False)]['latency'] > exc_lat\n",
    "    excl_cond = (excl_cond1 == True) & (excl_cond2 == True)\n",
    "    try: excl_num = excl_cond.value_counts().loc[True]\n",
    "    except: excl_num = 0\n",
    "    excl_rate = excl_num/total_pkg_num\n",
    "\n",
    "    return loss_rate, excl_rate\n",
    "\n",
    "# Convenience instance\n",
    "class EXPERIMENT():\n",
    "    def __init__(self, exp_dir_path, settings):\n",
    "        self.path = exp_dir_path\n",
    "        self.settings = json.loads(settings)\n",
    "    def __repr__(self):\n",
    "        return f'EXP: {self.path} -> {self.settings}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place give a XXXX-XX-XX.md file and find the experiment directory path\n",
    "# and the corresponding band settings. It will be presented by a list of special\n",
    "# instance EXPERIMENTs.\n",
    "\n",
    "md_file_path = '/Users/jackbedford/Desktop/MOXA/Code/data/2023-06-12/2023-06-12.md'\n",
    "date_dir_path = os.path.dirname(md_file_path)\n",
    "EXPs = []\n",
    "\n",
    "with open(md_file_path) as f:\n",
    "\n",
    "    exp = f.readline()[:-1]\n",
    "    settings = f.readline()[:-1]\n",
    "\n",
    "    while exp != '#endif' and settings:\n",
    "        E = EXPERIMENT(os.path.join(date_dir_path, exp), settings)\n",
    "        EXPs.append(E)\n",
    "        exp = f.readline()[:-1]\n",
    "        settings = f.readline()[:-1]\n",
    "\n",
    "EXPs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Radio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the example of given a experiment directory and plot the \n",
    "# box plot of DL/UL loss/excessive latency. \n",
    "\n",
    "# Still need to revise here.\n",
    "EXP = EXPs[0]\n",
    "exp_dir_path = EXP.path\n",
    "settings = EXP.settings \n",
    "\n",
    "dev_dir_list = find_device_under_exp(exp_dir_path)\n",
    "dev_metric_dict = {}\n",
    "\n",
    "for dev_dir_path in dev_dir_list:\n",
    "\n",
    "    dev = dev_dir_path.split('/')[-1]\n",
    "    metrics_dict = {}\n",
    "    trace_dir_list = find_trace_under_device(dev_dir_path)\n",
    "    dl_loss_rates, dl_excl_rates = [], []    \n",
    "    ul_loss_rates, ul_excl_rates = [], []\n",
    "\n",
    "    for trace_dir_path in trace_dir_list:\n",
    "\n",
    "        dl_file_path = os.path.join(trace_dir_path, 'data/udp_dnlk_loss_latency.csv')\n",
    "        ul_file_path = os.path.join(trace_dir_path, 'data/udp_uplk_loss_latency.csv')\n",
    "\n",
    "        dl_loss_rate, dl_excl_rate = count_loss_excl_rate(dl_file_path)\n",
    "        ul_loss_rate, ul_excl_rate = count_loss_excl_rate(ul_file_path)\n",
    "\n",
    "        dl_loss_rates.append(dl_loss_rate); dl_excl_rates.append(dl_excl_rate)\n",
    "        ul_loss_rates.append(ul_loss_rate); ul_excl_rates.append(ul_excl_rate)\n",
    "    \n",
    "    metrics_dict['dl_loss'] = dl_loss_rates\n",
    "    metrics_dict['dl_excl'] = dl_excl_rates\n",
    "    metrics_dict['ul_loss'] = ul_loss_rates\n",
    "    metrics_dict['ul_excl'] = ul_excl_rates\n",
    "\n",
    "    dev_metric_dict[dev] = metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(15, 8))\n",
    "dl_loss_boxes, dl_excl_boxes = [], []\n",
    "ul_loss_boxes, ul_excl_boxes = [], []\n",
    "labels = [settings[k] for k in list(dev_metric_dict.keys())]\n",
    "\n",
    "for k, v in dev_metric_dict.items():\n",
    "\n",
    "    dl_loss_box = [value*100 for value in dev_metric_dict[k]['dl_loss'] ] \n",
    "    dl_excl_box = [value*100 for value in dev_metric_dict[k]['dl_excl'] ] \n",
    "    ul_loss_box = [value*100 for value in dev_metric_dict[k]['ul_loss'] ]\n",
    "    ul_excl_box = [value*100 for value in dev_metric_dict[k]['ul_excl'] ]\n",
    "\n",
    "    dl_loss_boxes.append(dl_loss_box)\n",
    "    dl_excl_boxes.append(dl_excl_box)\n",
    "    ul_loss_boxes.append(ul_loss_box)\n",
    "    ul_excl_boxes.append(ul_excl_box)\n",
    "\n",
    "axes[0][0].boxplot(dl_loss_boxes, labels=labels)\n",
    "axes[0][0].set_title('DL Loss')\n",
    "axes[0][1].boxplot(dl_excl_boxes, labels=labels)\n",
    "axes[0][1].set_title('DL Excessive Latency')\n",
    "axes[1][0].boxplot(ul_loss_boxes, labels=labels)\n",
    "axes[1][0].set_title('UL Loss')\n",
    "axes[1][1].boxplot(ul_excl_boxes, labels=labels)\n",
    "axes[1][1].set_title('UL Excessive Latency')\n",
    "\n",
    "# fig.text(0.5, 0.0, 'common xlabel', ha='center', va='center', fontsize=16)\n",
    "fig.text(0.0, 0.5, 'Percentage %', ha='center', va='center', rotation='vertical', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place given an experiment directory, plot bar plot of UL/DL loss/excessive latency rate  \n",
    "# of all the traces. You must run the above box plot code to make this place work.\n",
    "\n",
    "dl_loss_bars, dl_excl_bars = [], []\n",
    "ul_loss_bars, ul_excl_bars = [], []\n",
    "trace_num = len(trace_dir_list)\n",
    "\n",
    "for i in range(trace_num):\n",
    "\n",
    "    dl_loss_bar = [dev_metric_dict[k]['dl_loss'][i]*100 for k in dev_metric_dict.keys()]\n",
    "    dl_excl_bar = [dev_metric_dict[k]['dl_excl'][i]*100 for k in dev_metric_dict.keys()]\n",
    "    ul_loss_bar = [dev_metric_dict[k]['ul_loss'][i]*100 for k in dev_metric_dict.keys()]\n",
    "    ul_excl_bar = [dev_metric_dict[k]['ul_excl'][i]*100 for k in dev_metric_dict.keys()]\n",
    "\n",
    "    dl_loss_bars.append(dl_loss_bar)\n",
    "    dl_excl_bars.append(dl_excl_bar)\n",
    "    ul_loss_bars.append(ul_loss_bar)\n",
    "    ul_excl_bars.append(ul_excl_bar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trace_num):\n",
    "    fig, axes = plt.subplots(2,2, figsize=(8, 4))\n",
    "\n",
    "    axes[0][0].bar(labels, dl_loss_bars[i], width=0.4)\n",
    "    axes[0][0].set_title('DL Loss')\n",
    "    axes[0][1].bar(labels, dl_excl_bars[i], width=0.4)\n",
    "    axes[0][1].set_title('DL Excessive Latency')\n",
    "    axes[1][0].bar(labels, ul_loss_bars[i], width=0.4)\n",
    "    axes[1][0].set_title('UL Loss')\n",
    "    axes[1][1].bar(labels, ul_excl_bars[i], width=0.4)\n",
    "    axes[1][1].set_title('UL Excessive Latency')\n",
    "\n",
    "    fig.text(0.0, 0.5, 'Percentage %', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Radio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place calculate the dual redio performance \n",
    "# given a experiment directory.\n",
    "\n",
    "# Still need to revise here.\n",
    "EXP = EXPs[0]\n",
    "exp_dir_path = EXP.path\n",
    "settings = EXP.settings \n",
    "\n",
    "dev_dir_list = find_device_under_exp(exp_dir_path)\n",
    "comb = itertools.combinations(dev_dir_list, 2)\n",
    "comb_metric_dict = {}\n",
    "\n",
    "for dev_dir_path1, dev_dir_path2 in comb:\n",
    "    \n",
    "    dev1 = dev_dir_path1.split('/')[-1]\n",
    "    dev2 = dev_dir_path2.split('/')[-1]\n",
    "    \n",
    "    print(dev1, dev2)\n",
    "    trace_dir_list1 = find_trace_under_device(dev_dir_path1)\n",
    "    trace_dir_list2 = find_trace_under_device(dev_dir_path2)\n",
    "    metrics_dict = {}\n",
    "\n",
    "    dl_loss_rates, dl_excl_rates = [], []    \n",
    "    ul_loss_rates, ul_excl_rates = [], []\n",
    "    \n",
    "    for trace_dir_path1, trace_dir_path2 in zip(trace_dir_list1, trace_dir_list2):\n",
    "        dl_file_path1 = os.path.join(trace_dir_path1, 'data/udp_dnlk_loss_latency.csv')\n",
    "        ul_file_path1 = os.path.join(trace_dir_path1, 'data/udp_uplk_loss_latency.csv')\n",
    "        dl_file_path2 = os.path.join(trace_dir_path2, 'data/udp_dnlk_loss_latency.csv')\n",
    "        ul_file_path2 = os.path.join(trace_dir_path2, 'data/udp_uplk_loss_latency.csv')\n",
    "\n",
    "        dl_loss_rate, dl_excl_rate = count_loss_excl_rate_dual(dl_file_path1, dl_file_path2)\n",
    "        ul_loss_rate, ul_excl_rate = count_loss_excl_rate_dual(ul_file_path1, ul_file_path2)\n",
    "\n",
    "        dl_loss_rates.append(dl_loss_rate); dl_excl_rates.append(dl_excl_rate)\n",
    "        ul_loss_rates.append(ul_loss_rate); ul_excl_rates.append(ul_excl_rate)\n",
    "    \n",
    "    metrics_dict['dl_loss'] = dl_loss_rates\n",
    "    metrics_dict['dl_excl'] = dl_excl_rates\n",
    "    metrics_dict['ul_loss'] = ul_loss_rates\n",
    "    metrics_dict['ul_excl'] = ul_excl_rates\n",
    "\n",
    "    comb_metric_dict[f'{dev1}+{dev2}'] = metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place calculate the average performance of an \n",
    "# experiment setting and plot the heatmap below.\n",
    " \n",
    "num_of_devs = len(settings)\n",
    "data_dl_loss = np.zeros((num_of_devs, num_of_devs))\n",
    "data_dl_excl = np.zeros((num_of_devs, num_of_devs))\n",
    "data_ul_loss = np.zeros((num_of_devs, num_of_devs))\n",
    "data_ul_excl = np.zeros((num_of_devs, num_of_devs))\n",
    "\n",
    "keys = list(comb_metric_dict.keys())\n",
    "count = 0\n",
    "\n",
    "for i in range(num_of_devs):\n",
    "    for j in range(i+1, num_of_devs):\n",
    "\n",
    "        k = keys[count]\n",
    "        \n",
    "        dl_loss = np.mean(comb_metric_dict[k]['dl_loss'])\n",
    "        dl_excl = np.mean(comb_metric_dict[k]['dl_excl'])\n",
    "        ul_loss = np.mean(comb_metric_dict[k]['ul_loss'])\n",
    "        ul_excl = np.mean(comb_metric_dict[k]['ul_excl'])\n",
    "\n",
    "        data_dl_loss[i, j] = dl_loss*100\n",
    "        data_dl_excl[i, j] = dl_excl*100\n",
    "        data_ul_loss[i, j] = ul_loss*100\n",
    "        data_ul_excl[i, j] = ul_excl*100\n",
    "\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(8,6))\n",
    "\n",
    "labels = list(settings.values())\n",
    "mask = np.tri(data_dl_loss.shape[0], dtype=bool, k=0)\n",
    "\n",
    "sns.heatmap(data_dl_loss, ax=axes[0,0], mask=mask, annot=True, fmt=\".2f\")\n",
    "axes[0,0].set_xticklabels(labels, rotation=90)\n",
    "axes[0,0].set_yticklabels(labels, rotation=0)\n",
    "axes[0,0].set_title('DL Loss')\n",
    "sns.heatmap(data_dl_excl, ax=axes[0,1], mask=mask, annot=True, fmt=\".2f\")\n",
    "axes[0,1].set_xticklabels(labels, rotation=90)\n",
    "axes[0,1].set_yticklabels(labels, rotation=0)\n",
    "axes[0,1].set_title('DL Excessive Latency')\n",
    "sns.heatmap(data_ul_loss, ax=axes[1,0], mask=mask, annot=True, fmt=\".2f\")\n",
    "axes[1,0].set_xticklabels(labels, rotation=90)\n",
    "axes[1,0].set_yticklabels(labels, rotation=0)\n",
    "axes[1,0].set_title('UL Loss')\n",
    "sns.heatmap(data_ul_excl, ax=axes[1,1], mask=mask, annot=True, fmt=\".2f\")\n",
    "axes[1,1].set_xticklabels(labels, rotation=90)\n",
    "axes[1,1].set_yticklabels(labels, rotation=0)\n",
    "axes[1,1].set_title('UL Excessive Latency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place plot the heatmap of every trace\n",
    "# of an experiment.\n",
    " \n",
    "num_of_devs = len(settings)\n",
    "keys = list(comb_metric_dict.keys())\n",
    "\n",
    "for t in range(trace_num):\n",
    "    \n",
    "    data_dl_loss = np.zeros((num_of_devs, num_of_devs))\n",
    "    data_dl_excl = np.zeros((num_of_devs, num_of_devs))\n",
    "    data_ul_loss = np.zeros((num_of_devs, num_of_devs))\n",
    "    data_ul_excl = np.zeros((num_of_devs, num_of_devs))\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i in range(num_of_devs):\n",
    "        for j in range(i+1, num_of_devs):\n",
    "\n",
    "            k = keys[count]\n",
    "            \n",
    "            dl_loss = comb_metric_dict[k]['dl_loss'][t]\n",
    "            dl_excl = comb_metric_dict[k]['dl_excl'][t]\n",
    "            ul_loss = comb_metric_dict[k]['ul_loss'][t]\n",
    "            ul_excl = comb_metric_dict[k]['ul_excl'][t]\n",
    "\n",
    "            data_dl_loss[i, j] = dl_loss*100\n",
    "            data_dl_excl[i, j] = dl_excl*100\n",
    "            data_ul_loss[i, j] = ul_loss*100\n",
    "            data_ul_excl[i, j] = ul_excl*100\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    fig, axes = plt.subplots(2,2, figsize=(8,6))\n",
    "\n",
    "    labels = list(settings.values())\n",
    "    mask = np.tri(data_dl_loss.shape[0], dtype=bool, k=0)\n",
    "\n",
    "    sns.heatmap(data_dl_loss, ax=axes[0,0], mask=mask, annot=True, fmt=\".2f\")\n",
    "    axes[0,0].set_xticklabels(labels, rotation=90)\n",
    "    axes[0,0].set_yticklabels(labels, rotation=0)\n",
    "    axes[0,0].set_title('DL Loss')\n",
    "    sns.heatmap(data_dl_excl, ax=axes[0,1], mask=mask, annot=True, fmt=\".2f\")\n",
    "    axes[0,1].set_xticklabels(labels, rotation=90)\n",
    "    axes[0,1].set_yticklabels(labels, rotation=0)\n",
    "    axes[0,1].set_title('DL Excessive Latency')\n",
    "    sns.heatmap(data_ul_loss, ax=axes[1,0], mask=mask, annot=True, fmt=\".2f\")\n",
    "    axes[1,0].set_xticklabels(labels, rotation=90)\n",
    "    axes[1,0].set_yticklabels(labels, rotation=0)\n",
    "    axes[1,0].set_title('UL Loss')\n",
    "    sns.heatmap(data_ul_excl, ax=axes[1,1], mask=mask, annot=True, fmt=\".2f\")\n",
    "    axes[1,1].set_xticklabels(labels, rotation=90)\n",
    "    axes[1,1].set_yticklabels(labels, rotation=0)\n",
    "    axes[1,1].set_title('UL Excessive Latency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(dl_file_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/home/wmnlab/D/database/2023-06-12/Bandlock_8_Schemes_Phone/combo'\n",
    "L = sorted([os.path.join(dir_path, x, 'udp_dnlk_combo_statistics.csv') for x in os.listdir(dir_path)])\n",
    "\n",
    "for f in L:\n",
    "    df = pd.read_csv(f)\n",
    "    print(df['lost_All+B3'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
